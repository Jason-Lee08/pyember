{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:14:42,364 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 17:14:42,366 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:14:42,569 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:14:42,571 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registry initialized: <ember.core.registry.model.base.registry.model_registry.ModelRegistry object at 0x7f97272d8890>\n",
      "openai:gpt-4o-audio-preview-2024-12-17\n",
      "openai:dall-e-3\n",
      "openai:text-embedding-3-large\n",
      "openai:dall-e-2\n",
      "openai:gpt-4o-audio-preview-2024-10-01\n",
      "openai:gpt-4.1-mini\n",
      "openai:gpt-4.1-mini-2025-04-14\n",
      "openai:gpt-4.1-nano\n",
      "openai:gpt-4.1-nano-2025-04-14\n",
      "openai:gpt-4o-realtime-preview-2024-10-01\n",
      "openai:gpt-4o-realtime-preview\n",
      "openai:gpt-4\n",
      "openai:text-embedding-ada-002\n",
      "openai:gpt-4o-mini-audio-preview\n",
      "openai:gpt-4o-audio-preview\n",
      "openai:gpt-4o-mini-realtime-preview\n",
      "openai:gpt-4o-mini-realtime-preview-2024-12-17\n",
      "openai:gpt-4-0125-preview\n",
      "openai:gpt-3.5-turbo-instruct-0914\n",
      "openai:gpt-4o-mini-search-preview\n",
      "openai:gpt-4-turbo-preview\n",
      "openai:gpt-3.5-turbo-1106\n",
      "openai:gpt-4o-search-preview\n",
      "openai:gpt-4-turbo\n",
      "openai:gpt-4o-realtime-preview-2024-12-17\n",
      "openai:gpt-3.5-turbo-instruct\n",
      "openai:gpt-3.5-turbo\n",
      "openai:gpt-4o-mini-search-preview-2025-03-11\n",
      "openai:gpt-4o-2024-11-20\n",
      "openai:gpt-4.1\n",
      "openai:gpt-4.1-2025-04-14\n",
      "openai:gpt-4o-2024-05-13\n",
      "openai:gpt-4-turbo-2024-04-09\n",
      "openai:gpt-3.5-turbo-16k\n",
      "openai:gpt-4-1106-preview\n",
      "openai:gpt-4-0613\n",
      "openai:gpt-4.5-preview\n",
      "openai:gpt-4.5-preview-2025-02-27\n",
      "openai:gpt-4o-search-preview-2025-03-11\n",
      "openai:text-embedding-3-small\n",
      "openai:gpt-4o-mini-tts\n",
      "openai:gpt-4o\n",
      "openai:gpt-4o-mini\n",
      "openai:gpt-4o-2024-08-06\n",
      "openai:gpt-4o-transcribe\n",
      "openai:gpt-4o-mini-2024-07-18\n",
      "openai:gpt-4o-mini-transcribe\n",
      "openai:gpt-4o-mini-audio-preview-2024-12-17\n",
      "openai:gpt-3.5-turbo-0125\n",
      "deepmind:gemini-1.0-pro-vision-latest\n",
      "deepmind:gemini-pro-vision\n",
      "deepmind:gemini-1.5-pro-latest\n",
      "deepmind:gemini-1.5-pro-001\n",
      "deepmind:gemini-1.5-pro-002\n",
      "deepmind:gemini-1.5-pro\n",
      "deepmind:gemini-1.5-flash-latest\n",
      "deepmind:gemini-1.5-flash-001\n",
      "deepmind:gemini-1.5-flash-001-tuning\n",
      "deepmind:gemini-1.5-flash\n",
      "deepmind:gemini-1.5-flash-002\n",
      "deepmind:gemini-1.5-flash-8b\n",
      "deepmind:gemini-1.5-flash-8b-001\n",
      "deepmind:gemini-1.5-flash-8b-latest\n",
      "deepmind:gemini-1.5-flash-8b-exp-0827\n",
      "deepmind:gemini-1.5-flash-8b-exp-0924\n",
      "deepmind:gemini-2.5-pro-exp-03-25\n",
      "deepmind:gemini-2.5-pro-preview-03-25\n",
      "deepmind:gemini-2.0-flash-exp\n",
      "deepmind:gemini-2.0-flash\n",
      "deepmind:gemini-2.0-flash-001\n",
      "deepmind:gemini-2.0-flash-exp-image-generation\n",
      "deepmind:gemini-2.0-flash-lite-001\n",
      "deepmind:gemini-2.0-flash-lite\n",
      "deepmind:gemini-2.0-flash-lite-preview-02-05\n",
      "deepmind:gemini-2.0-flash-lite-preview\n",
      "deepmind:gemini-2.0-pro-exp\n",
      "deepmind:gemini-2.0-pro-exp-02-05\n",
      "deepmind:gemini-exp-1206\n",
      "deepmind:gemini-2.0-flash-thinking-exp-01-21\n",
      "deepmind:gemini-2.0-flash-thinking-exp\n",
      "deepmind:gemini-2.0-flash-thinking-exp-1219\n",
      "deepmind:learnlm-1.5-pro-experimental\n",
      "deepmind:gemma-3-1b-it\n",
      "deepmind:gemma-3-4b-it\n",
      "deepmind:gemma-3-12b-it\n",
      "deepmind:gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "from ember import initialize_ember\n",
    "import os\n",
    "from ember.core.registry.model.providers.deepmind.deepmind_provider import create_deepmind_embedding_model\n",
    "from random import sample\n",
    "from typing import ClassVar, Type\n",
    "\n",
    "from ember.core.types.ember_model import EmberModel, Field\n",
    "from ember.core.utils.eval.diversity_evaluators import *\n",
    "from ember.core.registry.operator.core.diversity_scorer import *\n",
    "import sys\n",
    "from ember.core.registry.model.model_module.lm import LMModule, LMModuleConfig\n",
    "from ember.core.registry.operator.core.ensemble import (\n",
    "        EnsembleOperatorInputs,\n",
    "        EnsembleOperatorOutputs\n",
    ")\n",
    "from ember.core.registry.operator.base._module import static_field\n",
    "\n",
    "model = create_deepmind_embedding_model()\n",
    "\n",
    "model.embed_text(\"hey how are you?\")\n",
    "\n",
    "service = initialize_ember()\n",
    "print(f\"Registry initialized: {service}\")\n",
    "\n",
    "# Check which models were discovered\n",
    "model_ids = service.list_models()\n",
    "for m in model_ids:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPrefixOperatorInputs(EmberModel):\n",
    "    \"\"\"Input model for MultiPrefixEnsembleOperator.\n",
    "\n",
    "    Attributes:\n",
    "        query: The query string to be processed by the operator.\n",
    "    \"\"\"\n",
    "\n",
    "    query: str = Field(description=\"The query to be processed by multiple LM modules\")\n",
    "\n",
    "\n",
    "class MultiPrefixOperatorOutputs(EmberModel):\n",
    "    \"\"\"Output model for MultiPrefixEnsembleOperator.\n",
    "\n",
    "    Attributes:\n",
    "        responses: The list of responses from different LM modules.\n",
    "    \"\"\"\n",
    "\n",
    "    responses: List[str] = Field(description=\"Responses from different LM modules\")\n",
    "\n",
    "\n",
    "class MultiPrefixEnsembleSpecification(Specification):\n",
    "    \"\"\"Specification for MultiPrefixEnsembleOperator.\"\"\"\n",
    "\n",
    "    input_model: Type[EmberModel] = MultiPrefixOperatorInputs\n",
    "    structured_output: Type[EmberModel] = MultiPrefixOperatorOutputs\n",
    "\n",
    "\n",
    "class MultiPrefixEnsembleOperator(\n",
    "    Operator[MultiPrefixOperatorInputs, MultiPrefixOperatorOutputs]\n",
    "):\n",
    "    \"\"\"Operator that applies different prefixes using multiple LM modules.\n",
    "\n",
    "    This operator randomly selects prefixes from a predefined list and applies them\n",
    "    to the user query before sending to different language model modules.\n",
    "    \"\"\"\n",
    "\n",
    "    specification: ClassVar[Specification] = MultiPrefixEnsembleSpecification()\n",
    "    lm_modules: List[LMModule]\n",
    "    prefixes: List[str]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lm_modules: List[LMModule],\n",
    "        prefixes: List[str],\n",
    "        name: str = \"MultiPrefixEnsemble\",\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes a MultiPrefixEnsembleOperator instance.\n",
    "\n",
    "        Args:\n",
    "            lm_modules: A list of language model callables.\n",
    "            prefixes: A list of prefix strings to be used for each LM call.\n",
    "            name: The name identifier for this operator instance.\n",
    "        \"\"\"\n",
    "        self.prefixes = prefixes\n",
    "        self.lm_modules = lm_modules\n",
    "\n",
    "    def forward(\n",
    "        self, *, inputs: MultiPrefixOperatorInputs\n",
    "    ) -> MultiPrefixOperatorOutputs:\n",
    "        \"\"\"Apply different prefixes to the query and process through LM modules.\n",
    "\n",
    "        Args:\n",
    "            inputs: Validated input data containing the query.\n",
    "\n",
    "        Returns:\n",
    "            Structured output containing responses from all LM modules.\n",
    "        \"\"\"\n",
    "        # Randomly select prefixes to match the number of LM modules\n",
    "        chosen_prefixes = sample(self.prefixes, len(self.lm_modules))\n",
    "\n",
    "        # Process each query with a different prefix through its LM module\n",
    "        responses = []\n",
    "        for prefix, lm in zip(chosen_prefixes, self.lm_modules):\n",
    "            # Generate prompt with prefix\n",
    "            prompt = f\"{prefix}\\n{inputs.query}\"\n",
    "\n",
    "            # Call LM module\n",
    "            response = lm(prompt=prompt)\n",
    "\n",
    "            # Get text from response\n",
    "            text = response if isinstance(response, str) else str(response)\n",
    "\n",
    "            # Ensure we have a valid string\n",
    "            text = text.strip() if text else \"\"\n",
    "\n",
    "            responses.append(text)\n",
    "\n",
    "        # Return structured output\n",
    "        return MultiPrefixOperatorOutputs(responses=responses)\n",
    "\n",
    "class EnsembleOperator(Operator[EnsembleOperatorInputs, EnsembleOperatorOutputs]):\n",
    "    \"\"\"Real implementation of the Ensemble Operator.\"\"\"\n",
    "\n",
    "    specification = Specification[EnsembleOperatorInputs, EnsembleOperatorOutputs](\n",
    "        input_model=EnsembleOperatorInputs, structured_output=EnsembleOperatorOutputs\n",
    "    )\n",
    "\n",
    "    # Define static fields\n",
    "    lm_modules: List[LMModule] = static_field()\n",
    "\n",
    "    def __init__(self, lm_modules: List[LMModule]):\n",
    "        \"\"\"Initialize with LM modules.\"\"\"\n",
    "        self.lm_modules = lm_modules\n",
    "\n",
    "    def forward(self, *, inputs: EnsembleOperatorInputs) -> EnsembleOperatorOutputs:\n",
    "        \"\"\"Execute query across all models.\"\"\"\n",
    "        rendered_prompt = self.specification.render_prompt(inputs=inputs)\n",
    "        responses = [lm(prompt=rendered_prompt) for lm in self.lm_modules]\n",
    "        return EnsembleOperatorOutputs(responses=responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:14:53,775 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 17:14:53,779 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:14:53,968 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:14:53,970 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai:gpt-4o-audio-preview-2024-12-17\n",
      "openai:dall-e-3\n",
      "openai:text-embedding-3-large\n",
      "openai:dall-e-2\n",
      "openai:gpt-4o-audio-preview-2024-10-01\n",
      "openai:gpt-4.1-mini\n",
      "openai:gpt-4.1-mini-2025-04-14\n",
      "openai:gpt-4.1-nano\n",
      "openai:gpt-4.1-nano-2025-04-14\n",
      "openai:gpt-4o-realtime-preview-2024-10-01\n",
      "openai:gpt-4o-realtime-preview\n",
      "openai:gpt-4\n",
      "openai:text-embedding-ada-002\n",
      "openai:gpt-4o-mini-audio-preview\n",
      "openai:gpt-4o-audio-preview\n",
      "openai:gpt-4o-mini-realtime-preview\n",
      "openai:gpt-4o-mini-realtime-preview-2024-12-17\n",
      "openai:gpt-4-0125-preview\n",
      "openai:gpt-3.5-turbo-instruct-0914\n",
      "openai:gpt-4o-mini-search-preview\n",
      "openai:gpt-4-turbo-preview\n",
      "openai:gpt-3.5-turbo-1106\n",
      "openai:gpt-4o-search-preview\n",
      "openai:gpt-4-turbo\n",
      "openai:gpt-4o-realtime-preview-2024-12-17\n",
      "openai:gpt-3.5-turbo-instruct\n",
      "openai:gpt-3.5-turbo\n",
      "openai:gpt-4o-mini-search-preview-2025-03-11\n",
      "openai:gpt-4o-2024-11-20\n",
      "openai:gpt-4.1\n",
      "openai:gpt-4.1-2025-04-14\n",
      "openai:gpt-4o-2024-05-13\n",
      "openai:gpt-4-turbo-2024-04-09\n",
      "openai:gpt-3.5-turbo-16k\n",
      "openai:gpt-4-1106-preview\n",
      "openai:gpt-4-0613\n",
      "openai:gpt-4.5-preview\n",
      "openai:gpt-4.5-preview-2025-02-27\n",
      "openai:gpt-4o-search-preview-2025-03-11\n",
      "openai:text-embedding-3-small\n",
      "openai:gpt-4o-mini-tts\n",
      "openai:gpt-4o\n",
      "openai:gpt-4o-mini\n",
      "openai:gpt-4o-2024-08-06\n",
      "openai:gpt-4o-transcribe\n",
      "openai:gpt-4o-mini-2024-07-18\n",
      "openai:gpt-4o-mini-transcribe\n",
      "openai:gpt-4o-mini-audio-preview-2024-12-17\n",
      "openai:gpt-3.5-turbo-0125\n",
      "deepmind:gemini-1.0-pro-vision-latest\n",
      "deepmind:gemini-pro-vision\n",
      "deepmind:gemini-1.5-pro-latest\n",
      "deepmind:gemini-1.5-pro-001\n",
      "deepmind:gemini-1.5-pro-002\n",
      "deepmind:gemini-1.5-pro\n",
      "deepmind:gemini-1.5-flash-latest\n",
      "deepmind:gemini-1.5-flash-001\n",
      "deepmind:gemini-1.5-flash-001-tuning\n",
      "deepmind:gemini-1.5-flash\n",
      "deepmind:gemini-1.5-flash-002\n",
      "deepmind:gemini-1.5-flash-8b\n",
      "deepmind:gemini-1.5-flash-8b-001\n",
      "deepmind:gemini-1.5-flash-8b-latest\n",
      "deepmind:gemini-1.5-flash-8b-exp-0827\n",
      "deepmind:gemini-1.5-flash-8b-exp-0924\n",
      "deepmind:gemini-2.5-pro-exp-03-25\n",
      "deepmind:gemini-2.5-pro-preview-03-25\n",
      "deepmind:gemini-2.0-flash-exp\n",
      "deepmind:gemini-2.0-flash\n",
      "deepmind:gemini-2.0-flash-001\n",
      "deepmind:gemini-2.0-flash-exp-image-generation\n",
      "deepmind:gemini-2.0-flash-lite-001\n",
      "deepmind:gemini-2.0-flash-lite\n",
      "deepmind:gemini-2.0-flash-lite-preview-02-05\n",
      "deepmind:gemini-2.0-flash-lite-preview\n",
      "deepmind:gemini-2.0-pro-exp\n",
      "deepmind:gemini-2.0-pro-exp-02-05\n",
      "deepmind:gemini-exp-1206\n",
      "deepmind:gemini-2.0-flash-thinking-exp-01-21\n",
      "deepmind:gemini-2.0-flash-thinking-exp\n",
      "deepmind:gemini-2.0-flash-thinking-exp-1219\n",
      "deepmind:learnlm-1.5-pro-experimental\n",
      "deepmind:gemma-3-1b-it\n",
      "deepmind:gemma-3-4b-it\n",
      "deepmind:gemma-3-12b-it\n",
      "deepmind:gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "from ember import initialize_ember\n",
    "\n",
    "service = initialize_ember()\n",
    "\n",
    "for model in service.list_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 04:50:23,359 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 04:50:23,363 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 04:50:23,570 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 04:50:23,571 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 04:50:23,941 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 04:50:23,942 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 04:50:24,230 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 04:50:24,232 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 04:50:24,475 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 04:50:24,476 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 04:50:24,692 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 04:50:24,695 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 04:50:24,734 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 04:50:24,737 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 04:50:24,956 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 04:50:24,958 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 04:50:25,128 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 04:50:25,131 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 04:50:25,338 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 04:50:25,340 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 04:50:25,422 [WARNING] ember.core.registry.model.base.registry.factory: Provider name case mismatch: 'Openai' vs 'OpenAI'. Using the registered provider.\n",
      "2025-04-15 04:50:29,093 [WARNING] ember.core.registry.model.base.registry.factory: Provider name case mismatch: 'Openai' vs 'OpenAI'. Using the registered provider.\n",
      "2025-04-15 04:50:32,086 [WARNING] ember.core.registry.model.base.registry.factory: Provider name case mismatch: 'Openai' vs 'OpenAI'. Using the registered provider.\n",
      "2025-04-15 04:50:37,473 [WARNING] ember.core.registry.model.base.registry.factory: Provider name case mismatch: 'Openai' vs 'OpenAI'. Using the registered provider.\n",
      "2025-04-15 04:50:43,239 [WARNING] ember.core.registry.model.base.registry.factory: Provider name case mismatch: 'Openai' vs 'OpenAI'. Using the registered provider.\n",
      "2025-04-15 04:51:13,237 [WARNING] ember.core.utils.eval.diversity_evaluators: DiversityCosineEvaluator isn't initialized with an embedding model Using default OpenAI embedding model instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[MultiPrefix] Overall diversity score: 0.8799680411815644\n",
      "\n",
      "[Ensemble (normal)] Overall diversity score: 0.9860054671764373\n",
      "\n",
      "\n",
      "Original query: \"How can we effectively combat climate change while balancing economic needs?\"\n",
      "\n",
      "Number of responses: 5\n",
      "\n",
      "Response 1:\n",
      "  Prefix: \"Analyze this from a scientific perspective:\"\n",
      "  Response: \"Addressing climate change while balancing economic needs is a complex challenge that requires a mult...\"\n",
      "\n",
      "Response 2:\n",
      "  Prefix: \"Consider this from a philosophical angle:\"\n",
      "  Response: \"Effectively combating climate change while balancing economic needs is a multifaceted challenge that...\"\n",
      "\n",
      "Response 3:\n",
      "  Prefix: \"Provide a practical approach to:\"\n",
      "  Response: \"Addressing climate change while balancing economic needs is a complex challenge that requires a mult...\"\n",
      "\n",
      "Response 4:\n",
      "  Prefix: \"Imagine a crazy answer to the following:\"\n",
      "  Response: \"To effectively combat climate change while balancing economic needs, we could launch an intergalacti...\"\n",
      "\n",
      "Response 5:\n",
      "  Prefix: \"You are a scientist with deep expertise in solving the following:\"\n",
      "  Response: \"Effectively combating climate change while balancing economic needs requires a multifaceted and inte...\"\n",
      "\n",
      "Note: In a real application, these responses would be further processed or aggregated.\n"
     ]
    }
   ],
   "source": [
    "def usage_example() -> None:\n",
    "    \"\"\"Demonstrates usage of MultiPrefixEnsembleOperator with distinct prefixes for each language model.\n",
    "\n",
    "    This function creates a MultiPrefixEnsembleOperator with example prefixes and language model modules,\n",
    "    constructs sample input, executes the operator, and prints the aggregated responses.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define example prefixes to guide different response styles\n",
    "    example_prefixes: List[str] = [\n",
    "        \"Analyze this from a scientific perspective:\",\n",
    "        \"Consider this from a philosophical angle:\",\n",
    "        \"Provide a practical approach to:\",\n",
    "        \"Imagine a crazy answer to the following:\",\n",
    "        \"You are a scientist with deep expertise in solving the following:\",\n",
    "    ]\n",
    "\n",
    "    # Create LM modules with different models for diversity\n",
    "    lm_modules = [\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-2.0-flash\", temperature=0.5, max_tokens=256\n",
    "            )\n",
    "        ), \n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"openai:gpt-4o\", temperature=0.7, max_tokens=256\n",
    "            )\n",
    "        ),\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-1.5-pro\", temperature=0.3, max_tokens=256\n",
    "            )\n",
    "        ),\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-1.5-pro\", temperature=0.3, max_tokens=256\n",
    "            )\n",
    "        ),\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-2.0-flash\", temperature=0.3, max_tokens=256\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Instantiate the operator with named parameters.\n",
    "    prefix_ensemble_operator: MultiPrefixEnsembleOperator = MultiPrefixEnsembleOperator(\n",
    "        lm_modules=lm_modules,\n",
    "        prefixes=example_prefixes,\n",
    "        name=\"MultiPrefixEnsembleExample\",\n",
    "    )\n",
    "\n",
    "    # Create input data with a more substantive query\n",
    "    inputs: MultiPrefixOperatorInputs = MultiPrefixOperatorInputs(\n",
    "        query=\"How can we effectively combat climate change while balancing economic needs?\"\n",
    "    )\n",
    "\n",
    "    # Execute the operator using __call__ with named parameters\n",
    "    result = prefix_ensemble_operator(inputs=inputs)\n",
    "\n",
    "    # Try this same exact setup on an ensemble that does not add prefixes\n",
    "    normal_ensemble_operator: EnsembleOperator = EnsembleOperator(\n",
    "        lm_modules=lm_modules\n",
    "    )\n",
    "\n",
    "    inputs: EnsembleOperatorInputs = EnsembleOperatorInputs(\n",
    "        query=\"How can we effectively combat climate change while balancing economic needs?\"\n",
    "    )\n",
    "\n",
    "    normal_result = normal_ensemble_operator(inputs=inputs)\n",
    "\n",
    "    # Evaluate using cosine similarity\n",
    "    evaluator = DiversityCosineSimilarityEvaluator(\n",
    "        embedding_model=create_deepmind_embedding_model(\"gemini-embedding-exp-03-07\")\n",
    "    )\n",
    "\n",
    "    diverse_scores = evaluator.evaluate(result.responses)\n",
    "    \n",
    "    print(f\"\\n\\n[MultiPrefix] Overall diversity score: {diverse_scores.score}\\n\")\n",
    "\n",
    "    normal_scores = evaluator.evaluate(normal_result.responses)\n",
    "    print(f\"[Ensemble (normal)] Overall diversity score: {normal_scores.score}\")\n",
    "    \n",
    "    # Display structured results\n",
    "    print(f'\\n\\nOriginal query: \"{inputs.query}\"')\n",
    "    print(f\"\\nNumber of responses: {len(result.responses)}\")\n",
    "\n",
    "    # Display each response with its corresponding prefix\n",
    "    for i, (prefix, response) in enumerate(zip(example_prefixes, result.responses), 1):\n",
    "        # Show the prefix and a truncated response for readability\n",
    "        truncated = response[:100] + \"...\" if len(response) > 100 else response\n",
    "        print(f\"\\nResponse {i}:\")\n",
    "        print(f'  Prefix: \"{prefix}\"')\n",
    "        print(f'  Response: \"{truncated}\"')\n",
    "\n",
    "    print(\n",
    "        \"\\nNote: In a real application, these responses would be further processed or aggregated.\"\n",
    "    )\n",
    "\n",
    "usage_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    HAS_VISUALIZATION = True\n",
    "except ImportError:\n",
    "    logger.warning(\"Matplotlib not available. Visualizations will be skipped.\")\n",
    "    HAS_VISUALIZATION = False\n",
    "\n",
    "from ember.api import data, models\n",
    "from ember.api.data import Dataset\n",
    "from ember.core.exceptions import GatedDatasetAuthenticationError\n",
    "from ember.core.utils.eval.evaluators import MultipleChoiceEvaluator\n",
    "from ember.core.utils.eval.numeric_answer import AIMEAnswerEvaluator\n",
    "\n",
    "def evaluate_gpqa(\n",
    "    models_config: List[Tuple[str, Any]], sample_size: int = 5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Evaluate models on GPQA dataset.\n",
    "\n",
    "    Args:\n",
    "        models_config: List of (name, model) tuples to evaluate\n",
    "        sample_size: Number of problems to sample\n",
    "\n",
    "    Returns:\n",
    "        Results dictionary\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluating models on GPQA dataset...\")\n",
    "\n",
    "    # try:\n",
    "        # Load dataset\n",
    "    gpqa_data = data(\"gpqa\")\n",
    "    problems = gpqa_data.sample(sample_size)\n",
    "\n",
    "    if len(problems) == 0:\n",
    "        logger.error(\"GPQA dataset loaded but contains no questions\")\n",
    "        return {\"success\": False, \"error\": \"Empty dataset\"}\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = MultipleChoiceEvaluator()\n",
    "\n",
    "    # Track results\n",
    "    results = {\"success\": True, \"model_results\": {}, \"problems\": []}\n",
    "\n",
    "    # Run evaluation\n",
    "    for i, problem in enumerate(problems):\n",
    "        logger.info(f\"\\nQuestion {i+1}: {problem.query[:100]}...\")\n",
    "        logger.info(f\"Choices: {list(problem.choices.keys())}\")\n",
    "        logger.info(f\"Expected answer: {problem.metadata['correct_answer']}\")\n",
    "\n",
    "        # Format prompt\n",
    "        prompt = problem.query + \"\\n\\n\"\n",
    "        for key, choice in problem.choices.items():\n",
    "            prompt += f\"{key}. {choice}\\n\"\n",
    "\n",
    "        problem_result = {\n",
    "            \"id\": problem.metadata.get(\"id\", f\"question_{i}\"),\n",
    "            \"query\": problem.query,\n",
    "            \"choices\": problem.choices,\n",
    "            \"answer\": problem.metadata[\"correct_answer\"],\n",
    "            \"subject\": problem.metadata.get(\"subject\", \"Unknown\"),\n",
    "            \"model_responses\": {},\n",
    "        }\n",
    "\n",
    "        # Evaluate each model\n",
    "        for name, model in models_config:\n",
    "            if name not in results[\"model_results\"]:\n",
    "                results[\"model_results\"][name] = {\n",
    "                    \"correct\": 0,\n",
    "                    \"total\": 0,\n",
    "                    \"time\": 0,\n",
    "                }\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = model(prompt=prompt)\n",
    "            inference_time = time.time() - start_time\n",
    "\n",
    "            result = evaluator.evaluate(\n",
    "                response, problem.metadata[\"correct_answer\"]\n",
    "            )\n",
    "\n",
    "            # Update results\n",
    "            problem_result[\"model_responses\"][name] = {\n",
    "                \"response\": (\n",
    "                    response[:200] + \"...\" if len(response) > 200 else response\n",
    "                ),\n",
    "                \"is_correct\": result.is_correct,\n",
    "                \"time\": inference_time,\n",
    "            }\n",
    "\n",
    "            if result.is_correct:\n",
    "                results[\"model_results\"][name][\"correct\"] += 1\n",
    "            results[\"model_results\"][name][\"total\"] += 1\n",
    "            results[\"model_results\"][name][\"time\"] += inference_time\n",
    "\n",
    "            logger.info(\n",
    "                f\"{name}: {'✓' if result.is_correct else '✗'} [{inference_time:.2f}s]\"\n",
    "            )\n",
    "\n",
    "        results[\"problems\"].append(problem_result)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    for name in results[\"model_results\"]:\n",
    "        model_data = results[\"model_results\"][name]\n",
    "        model_data[\"accuracy\"] = (\n",
    "            model_data[\"correct\"] / model_data[\"total\"]\n",
    "            if model_data[\"total\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "        model_data[\"avg_time\"] = (\n",
    "            model_data[\"time\"] / model_data[\"total\"]\n",
    "            if model_data[\"total\"] > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "    # except GatedDatasetAuthenticationError as e:\n",
    "    #     logger.error(f\"Authentication required: {e.recovery_hint}\")\n",
    "    #     logger.info(\n",
    "    #         \"Request access at: https://huggingface.co/datasets/Idavidrein/gpqa\"\n",
    "    #     )\n",
    "    #     return {\n",
    "    #         \"success\": False,\n",
    "    #         \"error\": \"Authentication required\",\n",
    "    #         \"recovery\": e.recovery_hint,\n",
    "    #     }\n",
    "    # except Exception as e:\n",
    "    #     logger.error(f\"GPQA evaluation error: {e}\")\n",
    "    #     return {\"success\": False, \"error\": str(e)}\n",
    "\n",
    "\n",
    "\n",
    "def visualize_comparison(all_results: Dict[str, Dict[str, Any]]) -> None:\n",
    "    \"\"\"Visualize performance comparison across datasets.\n",
    "\n",
    "    Args:\n",
    "        all_results: Dictionary with dataset results\n",
    "    \"\"\"\n",
    "    if not HAS_VISUALIZATION:\n",
    "        logger.warning(\"Skipping visualization - matplotlib not available\")\n",
    "        return\n",
    "\n",
    "    # Extract data for visualization\n",
    "    datasets = []\n",
    "    model_names = set()\n",
    "\n",
    "    # First, identify all models and valid datasets\n",
    "    for dataset_name, result in all_results.items():\n",
    "        if result.get(\"success\", False):\n",
    "            datasets.append(dataset_name)\n",
    "            for model in result.get(\"model_results\", {}):\n",
    "                model_names.add(model)\n",
    "\n",
    "    if not datasets or not model_names:\n",
    "        logger.warning(\"No valid data for visualization\")\n",
    "        return\n",
    "\n",
    "    # Convert to sorted lists for consistent ordering\n",
    "    datasets = sorted(datasets)\n",
    "    model_names = sorted(model_names)\n",
    "\n",
    "    # Create accuracy matrix\n",
    "    accuracy_data = []\n",
    "    time_data = []\n",
    "\n",
    "    for model in model_names:\n",
    "        model_accuracy = []\n",
    "        model_time = []\n",
    "        for dataset in datasets:\n",
    "            result = all_results.get(dataset, {})\n",
    "            if result.get(\"success\", False):\n",
    "                model_result = result.get(\"model_results\", {}).get(model, {})\n",
    "\n",
    "                # Handle Codeforces differently (solution rate vs accuracy)\n",
    "                if dataset.lower() == \"codeforces\":\n",
    "                    accuracy = model_result.get(\"solution_rate\", 0)\n",
    "                else:\n",
    "                    accuracy = model_result.get(\"accuracy\", 0)\n",
    "\n",
    "                model_accuracy.append(accuracy * 100)  # Convert to percentage\n",
    "                model_time.append(model_result.get(\"avg_time\", 0))\n",
    "            else:\n",
    "                model_accuracy.append(0)\n",
    "                model_time.append(0)\n",
    "\n",
    "        accuracy_data.append(model_accuracy)\n",
    "        time_data.append(model_time)\n",
    "\n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Performance comparison\n",
    "    bar_width = 0.8 / len(model_names)\n",
    "    x = np.arange(len(datasets))\n",
    "\n",
    "    for i, (model, accuracy) in enumerate(zip(model_names, accuracy_data)):\n",
    "        offset = (i - len(model_names) / 2 + 0.5) * bar_width\n",
    "        ax1.bar(x + offset, accuracy, bar_width, label=model)\n",
    "\n",
    "    # Customize performance chart\n",
    "    ax1.set_ylabel(\"Accuracy (%)\")\n",
    "    ax1.set_title(\"Model Performance by Dataset\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([d.upper() for d in datasets])\n",
    "    ax1.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=len(model_names))\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    ax1.set_ylim(0, 105)  # Make room for labels\n",
    "\n",
    "    # Add data labels\n",
    "    for i, model_data in enumerate(accuracy_data):\n",
    "        for j, v in enumerate(model_data):\n",
    "            offset = (i - len(model_names) / 2 + 0.5) * bar_width\n",
    "            ax1.text(\n",
    "                j + offset, v + 1, f\"{v:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=8\n",
    "            )\n",
    "\n",
    "    # Time comparison\n",
    "    for i, (model, times) in enumerate(zip(model_names, time_data)):\n",
    "        offset = (i - len(model_names) / 2 + 0.5) * bar_width\n",
    "        ax2.bar(x + offset, times, bar_width, label=model)\n",
    "\n",
    "    # Customize time chart\n",
    "    ax2.set_ylabel(\"Average Time (seconds)\")\n",
    "    ax2.set_title(\"Model Inference Time by Dataset\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([d.upper() for d in datasets])\n",
    "    ax2.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=len(model_names))\n",
    "    ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add data labels\n",
    "    for i, model_data in enumerate(time_data):\n",
    "        for j, v in enumerate(model_data):\n",
    "            offset = (i - len(model_names) / 2 + 0.5) * bar_width\n",
    "            ax2.text(\n",
    "                j + offset, v + 0.1, f\"{v:.1f}s\", ha=\"center\", va=\"bottom\", fontsize=8\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_benchmark_results.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    logger.info(\"Visualization saved to model_benchmark_results.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_results(dataset_name: str, results: Dict[str, Any]) -> None:\n",
    "    \"\"\"Display evaluation results.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of the dataset\n",
    "        results: Results dictionary\n",
    "    \"\"\"\n",
    "    if not results.get(\"success\", False):\n",
    "        logger.error(\n",
    "            f\"{dataset_name} evaluation failed: {results.get('error', 'Unknown error')}\"\n",
    "        )\n",
    "        if \"recovery\" in results:\n",
    "            logger.info(f\"Recovery: {results['recovery']}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"\\n----- {dataset_name} Results -----\")\n",
    "    logger.info(f\"Problems evaluated: {len(results.get('problems', []))}\")\n",
    "\n",
    "    model_results = results.get(\"model_results\", {})\n",
    "\n",
    "    # Display model accuracy\n",
    "    for name, data in model_results.items():\n",
    "        if dataset_name.lower() == \"codeforces\":\n",
    "            solution_rate = data.get(\"solution_rate\", 0)\n",
    "            solutions = data.get(\"solutions\", 0)\n",
    "            total = data.get(\"total\", 0)\n",
    "            avg_time = data.get(\"avg_time\", 0)\n",
    "            logger.info(\n",
    "                f\"{name}: {solutions}/{total} solutions generated ({solution_rate:.1%}) - {avg_time:.2f}s avg\"\n",
    "            )\n",
    "        else:\n",
    "            accuracy = data.get(\"accuracy\", 0)\n",
    "            correct = data.get(\"correct\", 0)\n",
    "            total = data.get(\"total\", 0)\n",
    "            avg_time = data.get(\"avg_time\", 0)\n",
    "            logger.info(\n",
    "                f\"{name}: {correct}/{total} correct ({accuracy:.1%}) - {avg_time:.2f}s avg\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:31:27,468 [DEBUG] ConfigManager: Loading configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:31:27,502 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:31:27,960 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:31:27,962 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 17:31:28,315 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 17:31:28,316 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:31:28,527 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:31:28,530 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 17:31:28,733 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 17:31:28,734 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:31:28,933 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:31:28,935 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 17:31:29,335 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 17:31:29,337 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:31:29,536 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:31:29,538 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 17:31:29,736 [DEBUG] ConfigManager: Loading configuration...\n",
      "2025-04-15 17:31:29,739 [DEBUG] ConfigManager: Configuration loaded successfully\n",
      "2025-04-15 17:31:29,966 [ERROR] ember.core.registry.model.providers.anthropic.anthropic_discovery: Error fetching Anthropic models via REST API: 401 Client Error: Unauthorized for url: https://api.anthropic.com/v1/models\n",
      "2025-04-15 17:31:29,968 [WARNING] ember.core.registry.model.providers.anthropic.anthropic_discovery: No fallback models provided - API discovery required\n",
      "2025-04-15 17:31:30,108 [INFO] __main__: Evaluating models on GPQA dataset...\n",
      "2025-04-15 17:31:30,109 [ERROR] __main__: GPQA evaluation error: 'module' object is not callable\n",
      "2025-04-15 17:31:30,110 [ERROR] __main__: GPQA evaluation failed: 'module' object is not callable\n",
      "2025-04-15 17:31:30,111 [ERROR] __main__: All evaluations failed\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/research/ember/ember_env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3557: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"Run the model benchmark on specialized datasets.\"\"\"\n",
    "    SAMPLES = 5\n",
    "\n",
    "    # Configure models to evaluate\n",
    "    # models_config = [\n",
    "    #     (\"gpt-4o\", models.openai.gpt4o()),\n",
    "    #     (\"claude-3-opus\", models.anthropic.claude_3_opus()),\n",
    "    #     (\"claude-3-sonnet\", models.anthropic.claude_3_sonnet()),\n",
    "    # ]\n",
    "\n",
    "    lm_modules = [\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-2.0-flash\", temperature=0.5, max_tokens=256\n",
    "            )\n",
    "        ), \n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"openai:gpt-4o\", temperature=0.7, max_tokens=256\n",
    "            )\n",
    "        ),\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-1.5-pro\", temperature=0.3, max_tokens=256\n",
    "            )\n",
    "        ),\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-1.5-pro\", temperature=0.3, max_tokens=256\n",
    "            )\n",
    "        ),\n",
    "        LMModule(\n",
    "            config=LMModuleConfig(\n",
    "                model_name=\"deepmind:gemini-2.0-flash\", temperature=0.3, max_tokens=256\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    example_prefixes: List[str] = [\n",
    "        \"Analyze this from a scientific perspective:\",\n",
    "        \"Consider this from a philosophical angle:\",\n",
    "        \"Provide a practical approach to:\",\n",
    "        \"Imagine a crazy answer to the following:\",\n",
    "        \"You are a scientist with deep expertise in solving the following:\",\n",
    "    ]\n",
    "\n",
    "    # Instantiate the operator with named parameters.\n",
    "    prefix_ensemble_operator: MultiPrefixEnsembleOperator = MultiPrefixEnsembleOperator(\n",
    "        lm_modules=lm_modules,\n",
    "        prefixes=example_prefixes,\n",
    "        name=\"MultiPrefixEnsembleExample\",\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Run evaluations\n",
    "    # results[\"AIME\"] = evaluate_aime(models_config, args.samples)\n",
    "    # display_results(\"AIME\", results[\"AIME\"])\n",
    "\n",
    "    results[\"GPQA\"] = evaluate_gpqa([(\"prefix ensemble\", prefix_ensemble_operator)], SAMPLES)\n",
    "    display_results(\"GPQA\", results[\"GPQA\"])\n",
    "\n",
    "    # if args.dataset in [\"codeforces\", \"all\"]:\n",
    "    #     results[\"Codeforces\"] = evaluate_codeforces(models_config, min(args.samples, 3))\n",
    "    #     display_results(\"Codeforces\", results[\"Codeforces\"])\n",
    "\n",
    "    # Generate visualization if multiple datasets were evaluated\n",
    "    if len(results) > 1:\n",
    "        visualize_comparison(results)\n",
    "\n",
    "    # Check if any evaluations succeeded\n",
    "    any_success = any(r.get(\"success\", False) for r in results.values())\n",
    "    if not any_success:\n",
    "        logger.error(\"All evaluations failed\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        logger.info(\"\\nBenchmark completed successfully\")\n",
    "        logger.info(\"See model_benchmark_results.png for visualization\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ember_env)",
   "language": "python",
   "name": "ember_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
